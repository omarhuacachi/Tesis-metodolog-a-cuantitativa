# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1026Js41MQxs2fzE9KB6zujHy6PQEjRlH
"""

import pandas as pd

# Load and clean df_idh
df_idh = pd.read_csv('IDH-2019.csv')
df_idh.drop_duplicates(inplace=True)
df_idh['Departamentos'] = df_idh['Departamentos'].str.lower()
df_idh.rename(columns={'Departamentos': 'departamentos', 'IDH': 'idh'}, inplace=True)

# Load and clean df_conflictos
df_conflictos = pd.read_csv('conflictos sociales-2019.csv')
df_conflictos.drop_duplicates(inplace=True)
df_conflictos['Departamentos'] = df_conflictos['Departamentos'].str.lower()
df_conflictos.rename(columns={'Departamentos': 'departamentos', '2019': '2019_conflictos'}, inplace=True)

# Load and clean df_corrupcion
df_corrupcion = pd.read_csv('percepción de corrupción-2019.csv')
df_corrupcion.drop_duplicates(inplace=True)
df_corrupcion['Departamentos'] = df_corrupcion['Departamentos'].str.lower()
df_corrupcion.rename(columns={'Departamentos': 'departamentos', '2019': '2019_corrupcion'}, inplace=True)

# Load and clean df_pobreza
df_pobreza = pd.read_csv('pobreza-2019.csv')
df_pobreza.drop_duplicates(inplace=True)
df_pobreza['Departamentos'] = df_pobreza['Departamentos'].str.lower()
df_pobreza.rename(columns={'Departamentos': 'departamentos', '2019': '2019_pobreza'}, inplace=True)

display(df_idh.head())
display(df_conflictos.head())
display(df_corrupcion.head())
display(df_pobreza.head())

df_unified = pd.merge(df_idh, df_conflictos, on='departamentos', how='outer')
df_unified = pd.merge(df_unified, df_corrupcion, on='departamentos', how='outer')
df_unified = pd.merge(df_unified, df_pobreza, on='departamentos', how='outer')
display(df_unified.head())

df_unified = df_unified.groupby('departamentos', as_index=False).first()
display(df_unified.head())

num_unique_departments = df_unified['departamentos'].nunique()
print(f"Number of unique departments in df_unified: {num_unique_departments}")

all_departments = df_unified['departamentos'].unique().tolist()

original_dataframes = {
    'df_idh': df_idh,
    'df_conflictos': df_conflictos,
    'df_corrupcion': df_corrupcion,
    'df_pobreza': df_pobreza
}

for df_name, original_df in original_dataframes.items():
    missing_departments = [dept for dept in all_departments if dept not in original_df['departamentos'].unique()]
    print(f"Missing departments in {df_name}: {missing_departments}")

# Standardize department names in original dataframes
df_idh['departamentos'] = df_idh['departamentos'].str.strip().str.replace('ancash', 'áncash')
df_conflictos['departamentos'] = df_conflictos['departamentos'].str.strip().str.replace('ancash', 'áncash')
df_corrupcion['departamentos'] = df_corrupcion['departamentos'].str.strip().str.replace('ancash', 'áncash')
df_pobreza['departamentos'] = df_pobreza['departamentos'].str.strip().str.replace('ancash', 'áncash')

# Merge again after standardizing names
df_unified = pd.merge(df_idh, df_conflictos, on='departamentos', how='outer')
df_unified = pd.merge(df_unified, df_corrupcion, on='departamentos', how='outer')
df_unified = pd.merge(df_unified, df_pobreza, on='departamentos', how='outer')

# Group by to handle potential duplicates after merging
df_unified = df_unified.groupby('departamentos', as_index=False).first()

# Re-check the number of unique departments and missing departments
num_unique_departments = df_unified['departamentos'].nunique()
print(f"Number of unique departments in df_unified after standardization and merge: {num_unique_departments}")

all_departments = df_unified['departamentos'].unique().tolist()

original_dataframes = {
    'df_idh': df_idh,
    'df_conflictos': df_conflictos,
    'df_corrupcion': df_corrupcion,
    'df_pobreza': df_pobreza
}

for df_name, original_df in original_dataframes.items():
    missing_departments = [dept for dept in all_departments if dept not in original_df['departamentos'].unique()]
    print(f"Missing departments in {df_name} after standardization: {missing_departments}")

print("Column names of df_unified:")
print(df_unified.columns.tolist())
print("\nMissing values per column in df_unified:")
print(df_unified.isnull().sum())
print("\nFirst 5 rows of df_unified:")
display(df_unified.head())
print("\nNumber of unique departments in df_unified:")
print(num_unique_departments)
print(f"\nConfirmation: The DataFrame contains data for {num_unique_departments} departments.")

df_unified.to_csv('datos_unificados_peru.csv', index=False)

df_peru = pd.read_csv('datos_unificados_peru.csv')

# Convert 'idh' to numeric, removing commas
df_peru['idh'] = df_peru['idh'].astype(str).str.replace(',', '').astype(float)

# Check data types of relevant columns
print(df_peru[['idh', '2019_conflictos', '2019_corrupcion', '2019_pobreza']].dtypes)

import seaborn as sns
import matplotlib.pyplot as plt

# Create the first scatter plot
plt.figure(figsize=(10, 6))
sns.regplot(x='2019_pobreza', y='2019_conflictos', data=df_peru)
plt.title('Pobreza vs Conflictos Sociales')

# Create the second scatter plot
plt.figure(figsize=(10, 6))
sns.regplot(x='2019_corrupcion', y='2019_conflictos', data=df_peru)
plt.title('Percepción de Corrupción vs Conflictos Sociales')

# Create the third scatter plot
plt.figure(figsize=(10, 6))
sns.regplot(x='idh', y='2019_conflictos', data=df_peru)
plt.title('IDH vs Conflictos Sociales')

# Display the plots
plt.tight_layout()
plt.show()

import statsmodels.api as sm

X = df_peru[['2019_pobreza', '2019_corrupcion', 'idh']]
y = df_peru['2019_conflictos']

X = sm.add_constant(X)

model = sm.OLS(y, X)
results = model.fit()

print(results.summary())

# Get predicted values from the model
y_pred = results.predict(X)

# Create a scatter plot of actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, y, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Línea de Referencia (y=x)') # Add diagonal line

plt.xlabel('Valores Predichos de Conflictos Sociales')
plt.ylabel('Valores Reales de Conflictos Sociales')
plt.title('Valores Reales vs. Predicciones del Modelo de Regresión Múltiple')
plt.legend()
plt.grid(True)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %pip install statsmodels

X_vif = df_peru[['2019_pobreza', '2019_corrupcion', 'idh']]
display(X_vif.head())

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Add a constant to the DataFrame for VIF calculation
X_vif_const = sm.add_constant(X_vif)

# Calculate VIF for each independent variable
vif_data = pd.DataFrame()
vif_data["feature"] = X_vif_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif_const.values, i)
                   for i in range(X_vif_const.shape[1])]

print("Variance Inflation Factor (VIF) for independent variables:")
print(vif_data)

# Check for multicollinearity based on common thresholds
print("\nAnalysis of Multicollinearity:")
for index, row in vif_data.iterrows():
    if row['feature'] != 'const':
        if row['VIF'] > 10:
            print(f"High multicollinearity detected for '{row['feature']}': VIF = {row['VIF']:.2f} (> 10)")
        elif row['VIF'] > 5:
            print(f"Moderate multicollinearity detected for '{row['feature']}': VIF = {row['VIF']:.2f} (> 5 and <= 10)")
        else:
            print(f"Low multicollinearity detected for '{row['feature']}': VIF = {row['VIF']:.2f} (<= 5)")

print("\nImplications for the multiple regression model:")
print("High or moderate multicollinearity can make it difficult to interpret the individual coefficients of the affected variables, as their effects are intertwined. It can also increase the standard errors of the coefficients, making them less statistically significant. The overall model fit (R-squared) might still be reliable, but the contribution of individual predictors becomes unclear.")

y_new = df_peru['2019_conflictos']
X_new = df_peru[['2019_pobreza', '2019_corrupcion']]
X_new = sm.add_constant(X_new)
display(X_new.head())
display(y_new.head())

model_new = sm.OLS(y_new, X_new)
results_new = model_new.fit()

print(results_new.summary())

print("Previous model (with idh):")
print(f"R-squared: {results.rsquared:.3f}")
print("\nP-values of independent variables:")
print(results.pvalues[['2019_pobreza', '2019_corrupcion', 'idh']])

print("\nNew model (without idh):")
print(f"R-squared: {results_new.rsquared:.3f}")
print("\nP-values of independent variables:")
print(results_new.pvalues[['2019_pobreza', '2019_corrupcion']])

print("\nComparison:")
print(f"The previous model (with idh) has an R-squared of {results.rsquared:.3f}, while the new model (without idh) has an R-squared of {results_new.rsquared:.3f}.")
print("This indicates that the previous model explains slightly more variance in '2019_conflictos'.")

print("\nSignificance of independent variables (p-values < 0.05):")
print("Previous model:")
if results.pvalues['2019_pobreza'] < 0.05:
    print("- '2019_pobreza' is statistically significant.")
else:
    print("- '2019_pobreza' is not statistically significant.")

if results.pvalues['2019_corrupcion'] < 0.05:
    print("- '2019_corrupcion' is statistically significant.")
else:
    print("- '2019_corrupcion' is not statistically significant.")

if results.pvalues['idh'] < 0.05:
    print("- 'idh' is statistically significant.")
else:
    print("- 'idh' is not statistically significant.")

print("\nNew model:")
if results_new.pvalues['2019_pobreza'] < 0.05:
    print("- '2019_pobreza' is statistically significant.")
else:
    print("- '2019_pobreza' is not statistically significant.")

if results_new.pvalues['2019_corrupcion'] < 0.05:
    print("- '2019_corrupcion' is statistically significant.")
else:
    print("- '2019_corrupcion' is not statistically significant.")

# Get predicted values from the new model (without idh)
y_pred_new = results_new.predict(X_new)

# Create a scatter plot of actual vs predicted values for the new model
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_new, y_new, alpha=0.7)
plt.plot([y_new.min(), y_new.max()], [y_new.min(), y_new.max()], 'k--', lw=2, label='Línea de Referencia (y=x)') # Add diagonal line

plt.xlabel('Valores Predichos (Modelo sin IDH)')
plt.ylabel('Valores Reales de Conflictos Sociales')
plt.title('Valores Reales vs. Predicciones del Modelo (sin IDH)')
plt.legend()
plt.grid(True)
plt.show()

columns_to_analyze = ['2019_conflictos', '2019_pobreza', 'idh', '2019_corrupcion']
df_analysis = df_peru[columns_to_analyze]
display(df_analysis.head())

print("Basic statistics for the selected variables:")
display(df_analysis.describe(percentiles=[.25, .5, .75]))

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure size and layout for the plots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

# Create a histogram and a boxplot for each variable
for i, col in enumerate(columns_to_analyze):
    sns.histplot(data=df_analysis, x=col, ax=axes[i], kde=True)
    axes[i].set_title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, col in enumerate(columns_to_analyze):
    sns.boxplot(data=df_analysis, x=col, ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

# Calculate the correlation matrix
correlation_matrix = df_analysis.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación de Variables')
plt.show()